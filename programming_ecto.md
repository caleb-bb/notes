# Ecto 

## Ecto.Query

`Ecto.Query` is basically a DSL to write Elixir-y SQL queries. It does this by defining a query as a variable and then passing that query to `Repo`. There are two different syntaxes for this. The first is like so:

    query = from t in "tracks"
    join: a in "albums", on t.album_id = a.id
    where: t.duration > 900
    select: [t.id, t.title, a.title]

This is keyword syntax. If you want it to look more Elixir-y, you can write it using macro syntax, like so:

    query = "tracks"
    |> join(:inner, [t], a in "albums", on: t.album_id == a.id)
    |> where([t, a], t.duration > 900)
    |> select([t, a], [t.id, t.title, a.title])
    
You can then query the db by calling `Repo.all(query)`. If values are being passed into functions as different data types and we need to cast them, we can do so using `type(^thing_we_want_to_cast, :type_we_want_to_cast_it_as)`. For example, `type(:id, :integer)` could take `"1"` and make it into an integer, `1`.

We can see exactly what SQL is generated by `Ecto.Query` by passing it to `Ecto.Adapters.SQL.to_sql(:all, Repo, q)` where q is the query we want to investigate.


## Schema
The `schema ... do` expression defines a struct, fundamentally, but it goes further and specifies what data type is accepted by each struct field, and it does this for the purpose of mapping things to the schema. This expression uses the `schema` macro, which is granted courtesy of `use Ecto.Schema`. Then `field/2` takes the  names of fields and their data types. We don't need a one-to-one mapping with the table in the db. We can just specify the fields we want in this schema. 

`timestamps/0` grants `inserted_at` and `updated_at` fields, which Ecto automatically updates for every update and insertion of a record.

We don't need to include an `id` column among the calls to `field/2`, since Ecto automatically generates an `id` column and sets it as our primary key. Most databases have an `is` column which is assumed to be the primary key. This, however, is not universally consistent. If our db uses something else as a primary key, say, `thing_id` in table `things`, then we want:

    field :thing_id, :id, primary_key: true

which tells Ecto that there's a field called `:thing_id` which is the primary key for this schema. `:id` indicates an integer-based primary key, and `:uuid` indicates a primary key using Ecto's UUID utility.

### Types
Ecto has 15 types, which correspond to Elixir types. The Ecto types are on the left. The Elixir types are on the right.

#### Ecto Types <-> Elixir Types
- :id <-> integer
- :binary\_id <-> binary
- :integer <-> integer
- :float <-> float 
- :boolean <-> boolean
- :string <-> UTF-8 Encoded String
- :binary <-> binary
- {:array, inner\_type} <-> list
- {:map} <-> map 
- :decimal <-> Decimal 
- :date <-> Date
- :time <-> Time 
- :naive_datetime <-> NaiveDateTime 
- :utc_datetime <-> DateTime

The `map` type is dealt with differently by different databases. MySQL, for example, will store them as text fields, but Postgres has first-class support for JSON blobs and they are fully queryable. We like our maps to have string keys not atoms. When retrieving maps, Ecto always gives you back string keys. Ecto also provides an API to let us define custom types. 
  
Defining a query using a `select[...]` expression will return a struct of the appropriate type, but the struct fields will all be nil except those specified in the select statement.

Schemas, generally speaking, are meant to return data in a shape you can re-use. If you're not returning data that is meant to be re-used in its present shape, then simply writing a query is better than creating a schema. If you've got a database for dogs, then the %Dog{} struct that lists age, weight, breed, and sex is one that you're going to use over and over again, and it makes sense to write a schema that pulls together a struct with that shape out of the database.

### Associations
A foreign key connects two relations at the database level. Associations are there to model the foreign-key connections between tables. Recall that a table is a relation. 

Associations are things like has\_many, has\_one, and so on. When we write e.g. many\_to\_many, we are making a statement about the presence of foreign keys in a particular table or tables.

When we specify an association on a schema, we are saying that that field on the schema will contain structs of the given type. So, suppose we have a `School` schema with many `Students`. When we say

    has\_many :students, DB.School

We are telling ecto that our `%School{}` schema will have a field called `students`, which will contain many `%Student{}` structs. This is where preloading comes in: since `students` is a different table from `school`, we don't want to step through and load all the students because we might not want a struct that big. Instead, we might see that the Ecto association has not been loaded when we retrieve a school. This is why we go through all the `Ecto.preload(thing)` rigmarole. There is a feature in some other technologies called "lazy loading", where, when we try to refer to a record, the system checks if that record is loaded, and if it's not, then it preloads it for us automatically. This sounds nice, but it triggers a crap-ton of SQL queries. DB access being a major performance bottleneck, we prefer to avoid this.

In this case, `%School{}` is the parent record and the various instances of `%Student{}` are the child records. Ecto automatically generates the name of the foreign key using the name of the schema in which the association is defined. If this comes out incorrect for any reason, we can specify the foreign key manually in the schema definition, e.g. `has_many :things, DB.Thing, foreign_key: :weird_non_standard_id`.

`belongs_to/3` is always the child side of the `has_*` associations. It is the one side of the many-to-one and the child side of the one-to-one. More precisely, at the db level, `belongs_to/3` goes on the schema modeling the table that has a foreign key. In this example, `belongs_to/3` would go onto the student table, because each student has the foreign key of a school in their table. 

The `through: [...]` option of `has_many` takes a list of steps to get to the thing that the current schemas has many of. So if a school has many students has many classes, then we'd have something like `has_many :classes, through: [:students, :classes]`. It's like a trail of breadcrumbs to let the schema get things from another table. We ought to limit this to two or three levels. Now, `belongs_to` is not supported for nested associations. We can't create an association back from `:class` up to `:school`. We can, however, put a `many_to_many` association on both of them and use a join table. When we put a many-to-many association on two schemas that relate to one another, we use the `join_through:` option to feed that association a join table it can use. Because the join table is fed to both schemas on either side of the many-to-many association, we don't actually need a schema for the join table. It can live entirely in the background, and only manifest in our persistence framework as an argument to `join_through:`.

When defining a query, we can use `assoc/2` to refer to an association defined in our schema. For example, if we have

    query = from s in School,
        join: c in assoc(s, :classes)
        where c.name == "Science 101",
        preload: [classes: c]

And then run

    query
        |> Repo.all
        
We get a `%School{}` struct with `classes` preloaded. Because of the `has_many` association defined in the `School` schema, the resulting `%School{}` struct has a `classes` field. The line `join: c in assoc(s, classes)` performs a join on all classes associated with the school through its has-many association. Finally, the `preload: [classes: c]` tells Ecto to take c (which is all of the classes for the school) and load it as the value of the `classes` key in a list. 

As far as I can tell, `assoc(binding, :entities)` looks at the association type that has been defined on the schema for the query in which it occurs and infers what it should do on that basis.

### Embedded Schemas
An embedded schema is a way of making sure that child records are always loaded alongside a parent record, with no need to preload. Basically, an embedded schema lets you take an entire record, convert it to a string or a JSON blob, and cram it all into one column in the parent table.

Note that embedded schemas solve a similar problem to the one solved by join tables. In both cases, solving the problem without creating another table would require continuously adding new columns to a single table. The join table gets around this by having a table consisting only of pairs of foreign keys; the embedded schema gets around it by making the entire child record into a blob and putting it in one column. Join tables let us create many-to-many associations, while embedded schemas let us preload associated child records every time without the need to invoke `preload`. 

### Deleting records using associations
Some child associations want to persist after the deletion of their parents. Some should be deleted with their parents, by getting rid of the entire child record. And some should remain in place, but have their foreign keys set to `null`.

Just as `has_many` has a `foreign_key:` option, it also has an `on_delete:` option. The three possible values for `on_delete` are:

- `:nothing`, which does nothing. This is the default behavior.
- `:nillify_all`, which goes through the child records and sets the foreign keys referring to their parents to `null`. They are thus no longer associated with any parent record.
- `:delete_all` which goes through and wipes out all child records, deleting them entirely.

Note that many databases, including postgres, allow for us to set behaviors on the db itself that specify this deletion behavior. If such behaviors exist on the database itself, then setting them in Ecto will have no effect. These behaviors can be set up in migrations when database tables are created. 

### Seeding With Schemas
Since `Repo.insert` can take a schema struct as an argument, we can use it to insert stuff into our db in order to seed. Importantly, we can also pass nested structs to `Repo.insert` that reflect our associations between tables, and `Repo` will happily create the correct records. For example, we can have:

    Repo.insert(
        %Artist{
            name: "King Crimson",
            albums: [
                %Album{
                    title: "In The Court Of The Crimson King"
                }
            ]
        }
    )

If there exists a one-to-many (that is, `has_many`) association from `Artist` to `Album` (with the corresponding `belongs_to`) then this command will insert an artist into the db, take its id, insert an album, and put the artist's id onto the album as a foreign key. This also works with deeply nested associations. If, for example, an `Album` were to have-many `Tracks`, we could nest the tracks inside of the album parent struct, like so: `%Album{tracksL {%Track{}, %Track{}}]}`

### Transactions
A transaction is meant to preserve database integrity. It does this by putting together a whole bunch of database operations as a unit, and then having them either succeed together or fail together. If any single transaction doesn't work, they all fail and none of them happen. So, for example, suppose I want to transfer ten dollars from my bank account to yours. In the transaction that occurs, my account has to go down by ten dollars, and yours must go up by ten. If either of these things do not happen, the whole deal is off.

Different databases have different means of implementing transactions. As a general rule, you do the operations in the transaction one at a time, and if any of them fail, you execute a "rollback" and undo all of the operations up to the failed one.

Ecto implements transactions via `Repo.transaction`. `Repo.transaction` can take one of two arguments, which can be a function or an `%Ecto.Multi{}` struct. The function executes db operations, while the `Ecto.Multi{}` struct contains a queue of operations to execute. 

The big difference between `insert/1` and `insert!/1` is that the former returns an error tuple on failure and the latter raises. We use `insert!/1` when we're passing an anonymous function to `Repo.transaction` because `Repo.transaction` rolls back changes if and only if the function passed to it raises an error. This concerns only the case where `Repo.transaction` is passed a function. If we return an `:error` tuple, it will not roll back changes. 

Functions provide a lot of, uh, functionality in this sense, because they let us build non-database functionality into our transactions. If, for example, we have an external search engine that we want to update as our database changes, transactions in Ecto can do that for us. However, there are two major problems. First, it's easy to get burnt, because we can easily break our db if we write our anonymous functions the wrong way. Second, anonymous functions are not composable.

`Ecto.Multi` is here to help with all that. It's a data structure that holds database operations, and `Repo.transaction` is set up to handle `%Ecto.multi{}` structs. We define a multi struct by newing up an empty one with `Multi.new/0`. Once done, we can pipe that empty struct through any number of `Multi.insert(:table, thing)` statements, or use something besides insert if we want to do other things. This pipeline efines a multi that we can pass to `Repo.transaction`. For example,

    artist = %Artist{name: "David Bowie"}
    multi = 
        Multi.new
        |> Multi.insert(:artist, artist)
        |> Multi.insert(:log, Log.changeset_for_insert(artist))
    Repo.transaction(multi)
    
Notice how a pipeline of functions from `Multi` is used to define a struct that we can pass to `Repo.transaction/1`. The Ecto team recommends that we begin by newing up a Multi struct using `Multi.new` rather than trying to define the new struct directly, because `Multi.new` ensures that the new struct is correctly initialized. The `Multi` module has a number of functions that mirror the db operation functions in `Repo`, including `insert`, `update`, and `delete`, among others. We don't actually touch the database until we send the `%Multi{}` struct to `Repo.transaction/1`. When we do so, we get back an `:ok` tuple with a map. The keys of that map are the names we passed into the Multi statements (e.g. `:artist`, `:log`, etc). If the transaction fails when we pass a Multi struct, we get a nice 4-tuple with the changes that were attempted and rolled back, what kind of a failure it was, and what information caused the failure.  This tuple works very nicely when we feed it into a case statement, because we can pattern match and display error messages (potentially on forms) that say precisely what went wrong and where. The final value of the tuple will show all the changes that occurred up until the error. Diagnostically, this appears to be extremely valuable. If that map is empty, then none of the changes were applied. One reason this can happen is that Ecto will refuse to run the transaction at all if any of the changesets are invalid. This is why we must use changesets with multi, instead of just bare structs, whenever possible. 

Keep in mind that multi can do non-db operations as well as transactions run with anon functions. `Multi.run/3` takes a multi struct for its first arg, the name of the current repo as an atom for its second arg and an anonymous function for its third, and that anon function can run whatever code we want. We can also use `Multi.run/5`, which takes a pre-existing multi struct, the name of the current repo, the module from which we want to call a function, the name of the function to be called, and a list of arguments, in that order. There is no `Multi.all` to mirror `Repo.all`, so we could use `Multi.run/5` to create such a function if we wished.

`Multi` is still under construction and its API may still undergo major changes. Therefore, the safest way to introspect is to use `Multi.to_list(multi_struct)` in order to see all the operations queued therein.

## Migrations
There are 3 main points here.

1 - When a new table is created, the migration creates a primary key automatically, in a field called `id`. If you want to change the name or type of that field, then you want to set `primary_key: false` to `create table`, which prevents the creation of the id field. Then, if you so desire, you can add a field and set it as the primary key and give it whatever type you want. See the docs on `add field` for how to do this. Note that some tables, such as join tables, ought not to have a primary key at all. In that case, simply set the primary key to false in `create table` and then don't bother creating a primary key.

2 - When Ecto runs migrations, it sets up your db. But when Ecto rolls migrations back, it has to infer how to undo those changes. In some cases, this is relatively simple: `create index` goes to `drop index` and `drop index` goes to `drop table`. However, certain actions, such as deleting a table column and moving it to another table, cannot be reversed because Ecto doesn't know what to replace the deleted table with. In these cases, we use the `up` and `down` functions to tell Ecto what to do in those specific cases. It's worthwhile to roll back our migrations before we push to source control just to confirm that Ecto can infer what it's supposed to do.

3 - Migrations are run all at the same time. So, for example, if we create a table and then attempt to act on that table in the same migration, our migration will fail. This is because the table doesn't exist yet, and Ecto sees that our migration tries to act on a nonexistent table. The solution here is to create the table and then call `flush/0`. This function breaks the migration up into pieces that are executed sequentially; all code before a given `flush/0` call will be executed before the code that comes after. This lets us do cool stuff, like creating a table and then acting on it in the same migration.

## Changesets
The changeset is defiend using a `changeset` function in the schema. The controller then calls teh changeset function from the schema to create a new changeset, and passes that (along with conn) to `render/2`, like so:

    render(conn, changeset: changeset)

This will be taken up in the `.heex.html` file by a `form_for` function enclosed in the embedded Elixir tags. The first argument to form_for will be a changeset assign `@changeset` which is the changeset created by the function in the controller. `form_for` expects a `Phoenix.HTML.FormData` for its first argument. This first argument is a behavior, and the `phoenix_html` package provides an implementation of that behavior that can make changesets act like `FormData`. The second path to `form_for` is the so-called action URL. This is where the request goes when someone submits the form. In vanilla Phoenix, we'll usee `user_path(@conn, :action)` as the second argument to a form_for. In Liveview, however, things work a bit differently. Instead of `user_path` in our `form_for` we may see, e.g. `action={Routes.some_route(@conn, :action)}` which uses the `Routes` module to generate our action URL. Notice that the conn is passed in as an assign with the desired action as the second argument.

By default, Phoenix gives us HTML input names which are indexed by the schema names. So the fields we define on our schema, like this

    schema "some_schema" docs
        field :a_field, :string
        field :another, :integer
        
Will automatically be added to the HTML form as input fields, and can be accessed by their names. The controller that receives the form submission will have a function defined for whatever action is specified on the form, so `:action` will point to a function in the controller called `:action`. The second argument of the function that receives the form submission will be `params`, which can be pattern matched in the function head to pull whatever params we want out of the struct. So remember: when you look at a controller and see params as a function argument, the struct being passed thereto is defined as a set of fields on a schema somewhere. Phoenix also proved an `inputs_for` function that words a lot like `form_for`. `inputs_for` can take a parent form as an argument instead of a changeset, plus the name of an association to the schema for that form. The `inputs_for` can handle associations. So if there's an `embeds_one` association on the schema for the form being passed in, `inputs_for` will generate a sub-form for that association. If there's an `embeds_many`, then `inputs_for` is smart enough to generate as many sub-forms as there are records.
